#+LATEX_HEADER_EXTRA: \usepackage[scaled]{helvet}
#+LATEX_HEADER_EXTRA: \renewcommand\familydefault{\sfdefault}
#+LATEX_HEADER_EXTRA: \usepackage[T1]{fontenc}
#+LATEX_HEADER_EXTRA: \usepackage{tabularx}
#+LATEX_HEADER_EXTRA: \usepackage[left=2cm, right=2cm,top=2cm]{geometry}
#+LATEX_CLASS_OPTIONS: [15pt]
* Telescope
Kevin Poli, Phil Vitale, Connor O'Hara and Brendan Von Hofe
** Advisors:
Hong Man (hman@stevens.edu), Jeff Thompson (JThomps4@stevens.edu)
* Intro
Telescope is a machine learning assisted toolkit for digital video compositors
with applications in visual effects, matte painting and diverse use cases
accross the video post production pipeline. Tools from existing compositing
packages will interact with a novel ML core to assist or completely automate the
rotoscoping process. Rotoscoping is the process of masking and segmenting
poritons of an image accross multiple moving frames, any feature length movie
will often consist of hundreds of rotoscoped shots with multiple tracked mattes
per image, and this is the primary job of thousands of roto artists accross the world.

We hope to make this process, fast, intuitive, and accesible to alleviate the
manual and time consuming process that makes up a huge chunk of the man hours
required to produce even low budget features. We believe that machine learning
is in the process of revolutionizing image processing, and that user driven
toolkits rather than black box command line workflows will bring our intelligent
core into the hands of the artists where they can thrive.
** Demonstration
Rotoscoping is the process of frame by frame selecting and isolating a given feature (usually
an object or person) in a video, such that you can produce a video clip of
exclusively that selection on a transparent background

Lets walk through this step by step:

- First, our source image at frame 1, of Marceu the Mime
  [[./roto/Capture.PNG]]
- Lets start by creating a selection just of the Mime's face and hand - these
  are the features that are actually being "rotoscoped" out
  [[./roto/masked.PNG]]
- This purple selection represents a 'mask' which are the points and curves that
  make up the boundary of what we are looking to isolate. Traditionally, artists
  will digitally paint this selection in a software of their choice, by hand.
- This selection or mask is different from a matte, which is another important
  piece of terminology. A matte is a single channel image; meaning rather than
  pixels having red,green,blue values, they only contain 1 value from 0-255
  called 'alpha'. 'Alpha' will often be displayed in software as white. The
  Matte of this selection is an image where only the pixels corresponding to the
  selection are white, and all other pixels are black.
  [[./roto/matte.PNG]]
    - this is so that, under the hood, all we need to do is pixel-wise 'multiply' the
      source image to the matte, meaning any pixels with a black 'zero value' in
      the matte will become transparent, and any pixels in the white '255 value'
      in the matte will remain.
  [[./roto/goals.PNG]]
  - Here is the result of that multiply, an image containing only the pixels we
    selected before
*** Frame By Frame
Much of the challenge and tedium of rotoscoping comes from repeating the above
process for every frame, traditionally, artists will go frame by frame through
the video and manually adjust their selections to match the feature they are
isolating, here is the next frame of that video, with an adjusted selection for
clarity

  [[./roto/nextframe.PNG]]

to see how the selections should move as features in the video move, check out
this gif that displays the matte on the left, with the source on the right, and
has selection lines on both
https://www.google.com/url?sa=i&rct=j&q=&esrc=s&source=images&cd=&cad=rja&uact=8&ved=2ahUKEwj4poGei_bdAhVvTt8KHYSXBs0QjRx6BAgBEAU&url=https%3A%2F%2Ftaukeke.com%2F2014%2F07%2Frotoscoping-in-nuke%2F&psig=AOvVaw0rzB0nhBNxm_0WD1VdybtL&ust=1539062086451365
*** Use Cases
With our selection isolated, we can start to play with the image accordingly

By layering the source footage and our rotoscoped hand and face, we can apply an
effect, like the 'colorama' effect to only the pixels we roto'd previously

  [[./roto/isolated.PNG]]
**** Compositing
The most popular use case for rotoscoping is Compositing, which is the process
of combining multiple images into one. Consider three layers to see how this is
done.

Say we want this red square video clip to appear 'behind' the Mime's face and
hand (note what appears black is acutally transparent)

  [[./roto/red.PNG]]

We can grab our source clip and place the square image on top
  [[./roto/halfcomp.PNG]]
Then grab our rotoscoped face and hand and place that on top
  [[./roto/void.PNG]]
and here is the desired effect

  [[./roto/behind.PNG]]
* Technical Plan
** Components
Telescope as a product will consist of two primary modules, the Telescope Core,
which is a machine learning core assisted by traditional algorithmics that
implements the novel functionality of Telescope, and an exchange plugin that
allows existing professional compositing tools to interact with our proccesses.
Telescope For Nuke is our chosen example exhange plugin, designed to demonstrate
how the Telescope core can interact with existing artist workflows - but the
separation of core and plugin is designed such that Telescope can be implemented
into other software packages like Adobe After Effects or Blackmagic Design
Fusion at a later date.
| Category                     | What are we using?     |
|------------------------------+------------------------|
| Communication                |                        |
| Email                        | Gmail                  |
| Web Conferencing             | Facebook Video         |
| Instant Messaging            | GroupMe                |
| Collaboration                |                        |
| Document Collaboration       | Google Drive           |
| File Sharing/Data Tracking   | GitHub                 |
| Plugin Development           |                        |
| OS Supported                 | Windows, Mac OS, Linux |
| Host Application             | Nuke                   |
| Development Language         | C++                    |
| Machine Learning Development |                        |
| Development Language         | Python                 |
| Packages                     | PyTorch                |
** Algorithmics

The algorithmic core of our plugin will take images (frames of videos) as input and output segmentation masks (mattes) as output. The goal of the masks is to identify all the discrete objects in the image. It is class-agnostic and therefore does not need to determine what the objects are (e.g. cat or dog) but rather the fact that they are discrete.
Our criteria for determining how well our model is accomplishing the task is the Intersection-over-Union metric (IoU). We have yet to determine what an acceptable IoU score is for industry applications.
The model will be a convolutional neural network. Specifically, we will begin with the UNet model (https://arxiv.org/abs/1505.04597). Initially, our primary dataset to train the model with will be the Panoptic Detection COCO dataset, modified for a class-agnostic task.
Further iterations of the model will take advantage of the additional information in EXR images to refine object mattes and the DAVIS video object segmentation dataset.

** Dependency Model
#+BEGIN_center
#+ATTR_LATEX: :width 18cm :center nil
[[./Dgraph.pdf]]
#+END_center
** Plugin UI Mockups
[[./mockup.png]]

** Completion Schedule
For this semester, we hope to create a working plugin and corresponding ML core
that allows rotoscoping to be procedureally assisted on a single frame basis.
This does not mean that it will be impossible to rotoscope a video, just that
any given frame will not influence the matte of any other frame e.g matte
interpolation. Matte Interpolation via extending our ML core is our primary goal
for next semester along with testing and final implementations.
#+ATTR_LATEX: :environment tabularx :width \textwidth :align XXl
| Applications                                                                        | Machine Learning                                                                   | Week  |
|-------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+-------|
| TriMap Drawing Node                                                                 | Recreating Deep Image Matting Paper                                                | 10/14 |
|-------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+-------|
| Investigate Frame By Frame Refinement of TriMap Drawings                            | Continued Implementation, Begin Testing                                            | 10/21 |
|-------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+-------|
| Complete UI Skeleton for all Basic Tasks (TriMap Drawing, Selection, Interpolation) | Rigorously evaluate limits of model in realistic setting and research improvements | 10/28 |
|-------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+-------|
| Containerize Dependencies for Nuke ML Module Loader                                 | Dataset creation and augmentation improvements                                     | 11/4  |
|-------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+-------|
| Begin Developing ML Module Loader                                                   | Experiment with different model architectures                                      | 11/11 |
|-------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+-------|
| Integrate ML Module Loader into existing plugin Node                                | Explore new training schedules and perform hyperparameter tuning                   | 11/18 |
|-------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+-------|
| Test ML Module Loader on Various Models                                             | Begin research on frame-to-frame interpolation                                     | 11/25 |
|-------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+-------|
| Research TriMap Interpolation                                                       | Start implementation                                                               | 12/2  |
|-------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+-------|
| Implement Existing Matte to Curves output                                           | Finish implementation                                                              | 12/9  |
|-------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+-------|
| Finish up integrations for single frame implementation                              | Evaluate and improve                                                               | 12/6  |


* Team
** Roles 
- Connor O’Hara: Image Processing (cohara1@stevens.edu)
- Kevin Poli: Application/ Artist Tools Developer (kpoli@stevens.edu)
- Philip Vitale: Application & Systems Developer (pvitale@stevens.edu)
- Brendan von Hofe: Machine Learning (bvonhofe@stevens.edu)
  

 
** Delegation of Tasks

*** Connor O’Hara
**** Last Week
- Generative Ladder Networks deemed probably too bleeding edge to adopt, moved
  on to other tasks
**** Update
- Research into single channel matte to bezier curves has panned out
**** For Next week
- Stress test mattes -> curves, begin familiarity with PyTorch
*** Kevin Poli
**** Last Week
- UI paradigm mockups
**** Update
- see included mockup of multi matte browsing in one node
**** For Next Week
- UI paradigm and mockup for drawing and frame by frame editing TriMap garbage
  mattes
*** Phil Vitale
**** Last Week
- UI boilerplate, Educational Licensing
**** Update
- Ready to move into the actual Nuke application for Ui boilerplate, Educational
  Licensing for 1 Year has been secured for the whole team
**** For Next Week
- Basic matte painting node and actual skeleton of UI inside Nuke
*** Brendan Von Hofe
**** Last Week
- Procure the dataset from Deep Image Matting or being creating one
**** Update
- Gained access to the dataset used in the “Deep Image Matting” paper.
- Implemented data input pipeline and an encoder-decoder fully convolutional model in Pytorch. The specific flavor of encoder-decoder architecture is known as LinkNet (https://arxiv.org/pdf/1707.03718.pdf).
**** For Next Week
- Continue developing deep learning program to recreate “Deep Image Matting”
  paper.
  - Specifically, need to implement loss functions, model training schedule, evaluation script and the second (much smaller) convolutional network for matte refinement as described in the paper.
